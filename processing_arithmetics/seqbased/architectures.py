from keras.models import Model, load_model
from keras.layers import Embedding, Dense, Input, merge, SimpleRNN, GRU, LSTM, Masking
from keras.layers.wrappers import TimeDistributed
import keras.preprocessing.sequence
import sys
sys.path.insert(0, '../arithmetics') 
from arithmetics import mathTreebank
from TrainingHistory import TrainingHistory
from DrawWeights import DrawWeights
from PlotEmbeddings import PlotEmbeddings
import copy
from Logger import Logger
import matplotlib.pyplot as plt
import numpy as np
import random


class Training(object):
    # TODO write which functions a training class should implement
    """
    Give elaborate description
    functions that need to be implemented:
        - _build
        - init (set lossfunction and metrics)
        - train
    """
    def __init__(self, **kwargs):
        """
        Create training architecture
        """
        raise NotImplementedError

    def generate_model(self, recurrent_layer, input_dim, input_size, input_length,
                       size_hidden, dmap,
                       W_embeddings=None, W_recurrent=None, W_classifier=None,
                       train_classifier=True, train_embeddings=True, 
                       train_recurrent=True,
                       mask_zero=True, dropout_recurrent=0.0,
                       optimizer='adam',
                       extra_classifiers=None):
        """
        Generate the model to be trained
        :param recurrent_layer:     type of recurrent layer (from keras.layers SimpleRNN, GRU or LSTM)
        :param input_dim:           vocabulary size
        :param input_size:          dimensionality of the embeddings (input size recurrent layer)
        :param input_length:        max sequence length
        :param size_hidden:         size recurrent layer
        :param W_embeddings:        Either an embeddings matrix or None if to be generated by keras layer
        :param W_recurrent:         Either weights for the recurrent matrix or None if to be generated by keras layer
        :param W_classifier:        Either weights for the classifier or None if to be generated by keras layer
        :param dmap:                A map from vocabulary words to integers
        :param train_embeddings:    set to false to fix embedding weights during training
        :param train_classifier:    set to false to fix classifier layer weights during training
        :param train_recurrent:     set to false to fix recurrent weights during training
        :param mask_zero:           set to true to mask 0 values
        :param dropout_recurrent:   dropout param for recurrent weights
        :param optimizer:           optimizer to use during training
        :return:
        """

        # set network attributes
        self.recurrent_layer = recurrent_layer
        self.input_dim = input_dim
        self.input_size = input_size
        self.input_length = input_length
        self.size_hidden = size_hidden
        self.dmap = dmap
        self.train_classifier = train_classifier
        self.train_embeddings = train_embeddings
        self.train_recurrent = train_recurrent
        self.mask_zero = mask_zero
        self.dropout_recurrent = dropout_recurrent
        self.optimizer = optimizer
        self.trainings_history = None
        self.model = None
        self.classifiers = extra_classifiers


        # build model
        self._build(W_embeddings, W_recurrent, W_classifier)

    def _build(self, W_embeddings, W_recurrent, W_classifier):
        raise NotImplementedError()

    def add_pretrained_model(self, model, dmap, copy_weights=['recurrent','embeddings','classifier'], train_classifier=True, train_embeddings=True, train_recurrent=True, mask_zero=True, dropout_recurrent=0.0, optimizer='adam', classifiers=None, input_length=None):
        """
        Add a model with already trained weights. Model can be originally
        from a different training architecture, check which weights should be
        copied.
        :param model:           A keras model
        :param model_weights:   h5 file containing model weights
        :param optimizer:       optimizer to use during training
        :param copy_weights:    determines which weights should be copied
        """

        model_info = self.get_model_info(model)

        # find recurrent layer
        recurrent_layer = {'SimpleRNN': SimpleRNN, 'GRU': GRU, 'LSTM': LSTM}[model_info['recurrent_layer']]

        W_recurrent, W_embeddings, W_classifier = None, None, None

        # override weights with model weights
        if 'recurrent' in copy_weights:
            W_recurrent = model_info['weights_recurrent']

        if 'embeddings' in copy_weights:
            W_embeddings = model_info['weights_embeddings']

        if 'classifier' in copy_weights:
            W_classifier = model_info['classifiers']


        input_length = input_length if input_length else model_info['input_length']

        # run build function
        self.generate_model(recurrent_layer=recurrent_layer, input_dim=model_info['input_dim'], input_size=model_info['input_size'], input_length=input_length, size_hidden=model_info['size_hidden'], W_embeddings=W_embeddings, W_recurrent=W_recurrent, W_classifier=W_classifier, dmap=dmap, train_classifier=train_classifier, train_embeddings=train_embeddings, train_recurrent=train_recurrent, extra_classifiers=classifiers)
        return

    @staticmethod
    def generate_test_data(architecture, languages, dmap, digits, pad_to=None, test_separately=True, classifiers=None, format='infix'):
        """
        Take a dictionary that maps language names to number of sentences, and a list of classifiers for which to create test data. Return dictionary with classifier name as key and test data as output.
        :param languages:       dictionary mapping language names to numbers
        :param pad_to:          desired length of test sentences
        :return:                dictionary mapping classifier names to targets
        """

        if test_separately:
            test_data = []
            for name, N in languages.items():
                X, Y = architecture.generate_training_data({name: N}, dmap=dmap, digits=digits, classifiers=classifiers, pad_to=pad_to, format=format)
                test_data.append((name, X, Y))

        else:
            X, Y = architecture.generate_training_data(languages=languages, dmap=dmap, digits=digits, classifiers=classifiers, pad_to=pad_to, format=format)
            name = ', '.join(languages.keys())
            test_data = [(name, X, Y)]

        return test_data

    def train(self, training_data, batch_size, epochs, filename, validation_split=0.1, validation_data=None, sample_weight=None, verbosity=2, plot_embeddings=False, logger=False, save_every=False):
        """
        Fit the model.
        :param weights_animation:    Set to true to create an animation of the development of the embeddings
                                        after training.
        :param plot_embeddings:        Set to N to plot the embeddings every N epochs, only available for 2D
                                        embeddings.
        """
        X_train, Y_train = training_data

        callbacks = self.generate_callbacks(plot_embeddings, logger, recurrent_id=self.get_recurrent_layer_id(), embeddings_id=self.get_embeddings_layer_id(), save_every=save_every, filename=filename)

        sample_weight = self.get_sample_weights(training_data, sample_weight)

        # fit model
        self.model.fit(X_train, Y_train, validation_data=validation_data,
                       validation_split=validation_split, batch_size=batch_size, 
                       nb_epoch=epochs, sample_weight=sample_weight,
                       callbacks=callbacks, verbose=verbosity, shuffle=True)

        self.trainings_history = callbacks[0]            # set trainings_history as attribute

    def get_sample_weights(self, training_data, sample_weight):
        """
        Return a matrix with sample weights for the 
        input data if sample_weight parameter is true.
        """
        if not sample_weight:
            return None

        X_dict, Y_dict = training_data

        if len(X_dict) != 1:
            raise NotImplementedError("Number of inputs larger than 1, didn't think I'd need this case so I didn't implement it")

        sample_weights = {}
        for output in Y_dict:
            dim = Y_dict[output].ndim
            if dim == 2:
                # use sample_weight only for seq2seq models
                return None
            else:
                X_padded = X_dict.values()[0]
                sample_weight = np.zeros_like(X_padded)
                sample_weight[X_padded != 0] = 1
                sample_weights[output] = sample_weight

        return sample_weights



    def model_summary(self):
        print(self.model.summary())

    def get_model_info(self, model):
        """
        Get different type of weights from a json model. Check
        if network falls in family of networks that we are
        studying in this project: one layer recurrent neural
        networks that are trained in one of the architectures
        from this class A1 or A2.
        """

        # check if model is of correct type TODO
        n_layers = len(model.layers)

        # create list with layer objects

        model_info = {}
        model_info['classifiers'] = {}
        # loop through layers to get weight matrices

        for n_layer in xrange(n_layers):
            layer = model.layers[n_layer]
            layer_type = model.get_config()['layers'][n_layer]['class_name']
            weights = layer.get_weights()

            if layer_type == 'InputLayer':
                # decide on architecture by checking # of input layers
                if 'architecture' in model_info:
                    model_info['architecture'] = 'A4'
                else:
                    model_info['architecture'] = 'A1'

            elif layer_type == 'Embedding':
                assert 'weights_embeddings' not in model_info, "Model has too many embeddings layers"
                model_info['weights_embeddings'] = weights
                model_info['input_size'] = layer.get_config()['output_dim']
                model_info['input_dim'] = layer.get_config()['input_dim']
                model_info['input_length'] = layer.get_config()['input_length']

            elif layer_type in ['SimpleRNN', 'GRU', 'LSTM']:
                assert 'type' not in model_info, 'Model has too many recurrent layers' 
                model_info['recurrent_layer'] = layer_type
                model_info['weights_recurrent'] = weights
                model_info['size_hidden'] = layer.output_dim

            else:
                if weights != []:
                    model_info['classifiers'][layer.name] = weights
                
        return model_info

    def visualise_embeddings(self):
        raise NotImplementedError()

    def save_model(self, filename):
        """
        Save model to file
        """
        # check if filename exists
        exists = os.path.exists(filename+'.json')
        while exists:
            overwrite = raw_input("Filename exists, overwrite? (y/n)")
            if overwrite == 'y':
                exists = False
                continue
            filename = raw_input("Provide filename (without extension)")
              
        # save file
        model_json = self.model.to_json()
        open(filename+'.json', 'w').write(model_json)
        self.model.save_weights(filename+'_weights.h5')

    def plot_loss(self, save_to_file=False):
        """
        Plot loss on the last training
        of the network.
        """
        plt.plot(self.trainings_history.losses, label='Training set')
        plt.plot(self.trainings_history.val_losses, label='Validation set')
        plt.title("Loss during last training")
        plt.xlabel("Epoch")
        plt.ylabel(self.loss_function)
        plt.axhline(xmin=0)
        plt.legend()
        plt.show()

    def plot_metrics_training(self, save_to_file=False):
        """
        Plot the prediction error during the last training
        round of the network
        :param save_to_file:    file name to save file to
        """
        for metric in self.metrics:
            plt.plot(self.trainings_history.metrics_train[metric], label="%s training set" % metric)
            plt.plot(self.trainings_history.metrics_val[metric], label="%s validation set" % metric)

        plt.title("Monitors metrics during training")
        plt.xlabel("Epoch")
        plt.axhline(xmin=0)
        plt.ylabel("")
        plt.legend(loc=3)
        plt.show()

    def plot_esp(self):
        """
        Plot the spectral radius of the recurrent connections
        of the network during training
        """
        plt.plot(self.trainings_history.esp)
        plt.title("Spectral radius of recurrent connections")
        plt.xlabel("Epoch")
        plt.ylabel("spectral radius")
        plt.show()

    def plot_embeddings(self):
        """
        Plot embeddings of the network (only available for
        2 dimensional embeddings)
        :return:
        """
        weights = self.model.layers[1].get_weights()[0]
        assert weights.shape[1] == 2, "visualise embeddings only available for 2d embeddings"
        # find limits
        xmin, ymin = 1.1 * weights.max(axis=0)
        xmax, ymax = 1.1 * weights.min(axis=0)
        # use dmap to determine labels
        dmap_inverted = dict(zip(self.dmap.values(), self.dmap.keys()))
        for i in xrange(1, len(weights)):
            xy = tuple(weights[i])
            x, y = xy
            plt.plot(x, y, 'o')
            plt.annotate(dmap_inverted[i], xy=xy)
            plt.xlim([xmin,xmax])
            plt.ylim([ymin,ymax])
            i += 1
        plt.show()

    def generate_callbacks(self, plot_embeddings, print_every, recurrent_id, embeddings_id, save_every, filename):
        """
        Generate sequence of callbacks to use during training
        :param recurrent_id:
        :param weights_animation:        set to true to generate visualisation of embeddings
        :param plot_embeddings:             generate scatter plot of embeddings every plot_embeddings epochs
        :param print_every:                 print summary of results every print_every epochs
        :return:
        """

        history = TrainingHistory(metrics=self.metrics, recurrent_id=recurrent_id, param_id=1, save_every=save_every, filename=filename)
        callbacks = [history]

        if plot_embeddings:
            if plot_embeddings is True:
                pass
            else:
                embeddings_plot = PlotEmbeddings(plot_embeddings, self.dmap, embeddings_id=embeddings_id)
                callbacks.append(embeddings_plot)

        if print_every:
            logger = Logger(print_every)
            callbacks.append(logger)

        return callbacks

    def _build(self, W_embeddings, W_recurrent, W_classifier):
        raise NotImplementedError("Should be implemented in subclass")

    @staticmethod
    def generate_training_data(languages, dmap, digits, pad_to=None, format='infix', classifiers=None):
        raise NotImplementedError("Should be implemented in subclass")

    @staticmethod
    def data_from_treebank(treebank, dmap, pad_to=None, classifiers=None, format='infix'):
        raise NotImplementedError("Should be implemented in subclass")

    @staticmethod
    def get_embeddings_layer_id():
        raise NotImplementedError("Should be implemented in subclass")

    @staticmethod
    def get_recurrent_layer_id():
        raise NotImplementedError("Should be implemented in subclass")



class A1(Training):
    """
    Give description.
    """
    def __init__(self):
        self.loss_function = 'mean_squared_error'
        self.metrics = ['mean_absolute_error', 'mean_squared_error', 'binary_accuracy']

    def _build(self, W_embeddings, W_recurrent, W_classifier):
        """
        Build the trainings architecture around
        the model.
        """
        # create input layer
        input_layer = Input(shape=(self.input_length,), dtype='int32', name='input')

        # create embeddings
        embeddings = Embedding(input_dim=self.input_dim, output_dim=self.input_size,
                               input_length=self.input_length, weights=W_embeddings,
                               trainable=self.train_embeddings,
                               mask_zero=self.mask_zero,
                               name='embeddings')(input_layer)

        # create recurrent layer
        recurrent = self.recurrent_layer(self.size_hidden, name='recurrent_layer',
                                         weights=W_recurrent,
                                         trainable=self.train_embeddings,
                                         dropout_U=self.dropout_recurrent)(embeddings)

        # create output layer
        if W_classifier != None:
            W_classifier = W_classifier['output'] 
        output_layer = Dense(1, activation='linear', weights=W_classifier,
                             trainable=self.train_classifier, name='output')(recurrent)

        # create model
        self.model = Model(input=input_layer, output=output_layer)

        # compile
        self.model.compile(loss={'output': self.loss_function}, optimizer=self.optimizer,
                           metrics=self.metrics)

    @staticmethod
    def generate_training_data(languages, dmap, digits, pad_to=None, format='infix', classifiers=None):
        """
        Take a dictionary that maps languages to number of sentences and
         return numpy arrays with training data.
        :param languages:       dictionary mapping languages (str name) to numbers
        :param pad_to:          length to pad training data to
        :return:                tuple, input, output, number of digits, number of operators
                                map from input symbols to integers
        """
        # generate treebank with examples
        treebank = mathTreebank(languages, digits=digits)
        random.shuffle(treebank.examples)

        data = A1.data_from_treebank(treebank, dmap, pad_to=pad_to, format=format)

        return data

    @staticmethod
    def data_from_treebank(treebank, dmap, pad_to=None, classifiers=None, format='infix'):
        """
        Generate test data from a mathTreebank object.
        """
        X, Y = [], []
        for expression, answer in treebank.examples:
            str_expression = expression.toString(format).split()
            input_seq = [dmap[i] for i in expression.toString(format).split()]
            answer = answer
            X.append(input_seq)
            Y.append(answer)

        # pad sequences to have the same length
        assert pad_to is None or len(X[0]) <= pad_to, 'length test is %i, max length is %i. Test sequences should not be truncated' % (len(X[0]), pad_to)
        X_padded = keras.preprocessing.sequence.pad_sequences(X, dtype='int32', maxlen=pad_to)
        X = {'input':X_padded}
        Y = {'output':np.array(Y)}

        return X, Y

    @staticmethod
    def get_embeddings_layer_id():
        """
        Return embeddings layer ID
        :return: (int) id of embeddings layer
        """
        return 1


    @staticmethod
    def get_recurrent_layer_id():
        """
        Return recurrent layer ID
        :return: (int) id of recurrent layer
        """
        return 2


class A4(Training):
    """
    Give description.
    """
    def __init__(self):
        self.loss_function = 'categorical_crossentropy'
        # self.loss_function = 'mean_squared_error'

        self.metrics = ['categorical_accuracy']

    def _build(self, W_embeddings, W_recurrent, W_classifier={'output':None}):
        """
        Build the trainings architecture around
        the model.
        """
        # create input layer
        input1 = Input(shape=(self.input_length,), dtype='int32', name='input1')
        input2 = Input(shape=(self.input_length,), dtype='int32', name='input2')

        # create embeddings
        embeddings = Embedding(input_dim=self.input_dim, output_dim=self.input_size,
                               input_length=self.input_length, weights=W_embeddings,
                               mask_zero=self.mask_zero, trainable=self.train_embeddings,
                               name='embeddings')

        # create recurrent layer
        recurrent = self.recurrent_layer(self.size_hidden, name='recurrent_layer',
                                         weights=W_recurrent,
                                         trainable=self.train_recurrent,
                                         dropout_U=self.dropout_recurrent)

        embeddings1 = embeddings(input1)
        embeddings2 = embeddings(input2)

        recurrent1 = recurrent(embeddings1)
        recurrent2 = recurrent(embeddings2)

        concat = merge([recurrent1, recurrent2], mode='concat', concat_axis=-1)

        # create output layer
        if W_classifier != None:
            W_classifier = W_classifier['output']
        output_layer = Dense(3, activation='softmax', 
                             trainable=self.train_recurrent,
                             weights=W_classifier, name='output')(concat)

        # create model
        self.model = Model(input=[input1, input2], output=output_layer)

        # compile
        self.model.compile(loss={'output': self.loss_function}, optimizer=self.optimizer,
                           metrics=self.metrics)

    @staticmethod
    def generate_training_data(languages, dmap, digits, pad_to=None, format='infix', classifiers=None):
        """
        Take a dictionary that maps languages to number of sentences and
         return numpy arrays with training data.
        :param languages:       dictionary mapping languages (str name) to numbers
        :param pad_to:          length to pad training data to
        :return:                tuple, input, output, number of digits, number of operators
                                map from input symbols to integers
        """
        # generate treebank with examples
        treebank = mathTreebank(languages, digits=digits)

        X_padded, Y = A4.data_from_treebank(treebank, dmap=dmap, pad_to=pad_to, classifiers=None, format=format)


        return X_padded, Y

    @staticmethod
    def data_from_treebank(treebank, dmap, pad_to=None, classifiers=None, format='infix'):
        """
        Generate data from mathTreebank object.
        """
        # create empty input and targets
        X1, X2, Y = [], [], []

        # loop over examples
        for example1, example2, compare in treebank.pairedExamples:
            input_seq1 = [dmap[i] for i in example1.toString(format).split()]
            input_seq2 = [dmap[i] for i in example2.toString(format).split()]
            answer = np.zeros(3)
            answer[np.argmax([compare == '<', compare == '=',  compare == '>'])] = 1
            X1.append(input_seq1)
            X2.append(input_seq2)
            Y.append(answer)

        # pad sequences to have the same length
        assert pad_to is None or len(X1[0]) <= pad_to, 'length test is %i, max length is %i. Test sequences should not be truncated' % (len(X1[0]), pad_to)
        X1_padded = keras.preprocessing.sequence.pad_sequences(X1, dtype='int32', maxlen=pad_to)
        X2_padded = keras.preprocessing.sequence.pad_sequences(X2, dtype='int32', maxlen=pad_to)

        X_padded = {'input1':X1_padded, 'input2': X2_padded}
        Y = {'output': np.array(Y)}

        return X_padded, Y

    @staticmethod
    def get_embeddings_layer_id():
        """
        Return embeddings layer ID
        :return: (int) id of embeddings layer
        """
        return 2


    @staticmethod
    def get_recurrent_layer_id():
        """
        Return recurrent layer ID
        :return: (int) id of recurrent layer
        """
        return 3

class Seq2Seq(Training):
    """
    Class to do sequence to sequence training.
    """
    def __init__(self):
        # set loss function and metrics
        self.loss_function = 'mean_squared_error'
        self.metrics = ['mean_absolute_error', 'mean_squared_error', 'binary_accuracy']

    def _build(self, W_embeddings, W_recurrent, W_classifier):
        """
        Build model
        """

        # create input layer
        input_layer = Input(shape=(self.input_length,), dtype='int32', name='input')

        # create embeddings
        embeddings = Embedding(input_dim=self.input_dim, output_dim=self.input_size,
                               input_length=self.input_length, weights=W_embeddings,
                               trainable=True,
                               mask_zero=self.mask_zero,
                               name='embeddings')(input_layer)

        # create recurrent layer
        recurrent = self.recurrent_layer(self.size_hidden, name='recurrent_layer',
                                         weights=W_recurrent,
                                         trainable=True,
                                         return_sequences=True,
                                         dropout_U=self.dropout_recurrent)(embeddings)

        mask = TimeDistributed(Masking(mask_value=0.0))(recurrent)

        W_classifier = W_classifier['output']
        output = TimeDistributed(Dense(1, activation='linear'), name='output')(mask)

        self.model = Model(input=input_layer, output=output)

        self.model.compile(loss=self.loss_function, optimizer=self.optimizer, metrics=self.metrics, sample_weight_mode='temporal')

    @staticmethod
    def generate_training_data(languages, dmap, digits, classifiers=None, pad_to=None, format='infix'):

        # generate and shuffle examples
        treebank = mathTreebank(languages, digits=digits)
        random.shuffle(treebank.examples)

        X, Y = Seq2Seq.data_from_treebank(treebank, dmap, pad_to=pad_to, classifiers=None, format=format)

        return X, Y

    @staticmethod
    def data_from_treebank(treebank, dmap, pad_to, classifiers=None, format='infix'):
        """
        Generate test data from a mathTreebank object.
        """
        # create dictionary with outputs
        X, Y = [], []

        # loop over examples
        for expression, answer in treebank.examples:
            expression.get_targets()
            input_seq = [dmap[i] for i in expression.toString(format).split()]
            X.append(input_seq)
            Y.append(expression.targets['intermediate_locally'])

        # pad sequences to have the same length
        assert pad_to is None or len(X[0]) <= pad_to, 'length test is %i, max length is %i. Test sequences should not be truncated' % (len(X[0]), pad_to)
        X_padded = keras.preprocessing.sequence.pad_sequences(X, dtype='int32', maxlen=pad_to)
        Y_padded = keras.preprocessing.sequence.pad_sequences(Y, maxlen=pad_to, dtype='float32')

        X = {'input': X_padded}
        Y = {'output': Y_padded}


        return X, Y

    @staticmethod
    def get_embeddings_layer_id():
        """
        Return embeddings layer ID
        :return (int) id of embeddings layer
        """
        return 1


    @staticmethod
    def get_recurrent_layer_id():
        """
        return recurrent layer ID
        :return (int) id of recurrent layer
        """
        return 2


class Probing(Training):
    """
    Retrain an already trained model with new classifiers to
    test what information is extratable from the representations
    the model generates.
    """
    def __init__(self):
        """
        List with classifiers, create dictionaries with 
        corresponding metrics, loss function, activation functions
        and output sizes.
        """
        self.loss = {
                'grammatical': 'binary_crossentropy',
                'intermediate_locally': 'mean_squared_error',
                'subtracting':'binary_crossentropy',
                'intermediate_recursively':'mean_squared_error',
                'top_stack':'mean_squared_error_ignore'}

        self.metrics = {
                'grammatical': ['binary_accuracy'], 
                'intermediate_locally': ['mean_absolute_error', 'mean_squared_error', 'binary_accuracy'],
                'subtracting': ['binary_accuracy'],
                'intermediate_recursively': ['mean_absolute_error', 'mean_squared_error', 'binary_accuracy'],
                'top_stack': ['mean_squared_error_ignore', 'mean_squared_prediction_error_ignore']}  

        self.activations = {
                'grammatical':'sigmoid',
                'intermediate_locally': 'linear',
                'subtracting': 'sigmoid',
                'intermediate_recursively':'linear',
                'top_stack': 'linear'}

        self.output_size = {
                'grammatical':1,
                'intermediate_locally': 1,
                'subtracting':1,
                'intermediate_recursively':1,
                'top_stack':1}


    def _build(self, W_embeddings, W_recurrent, W_classifier):
        """
        Build model with given embeddings and recurren weights.
        """

        # set attributes
        self.set_attributes()

        # create input layer
        input_layer = Input(shape=(self.input_length,), dtype='int32', name='input')
        mask1 = Masking(mask_value=0.0)(input_layer)


        # create embeddings
        embeddings = Embedding(input_dim=self.input_dim, output_dim=self.input_size,
                               input_length=self.input_length, weights=W_embeddings,
                               trainable=False,
                               mask_zero=self.mask_zero,
                               name='embeddings')(mask1)

        # create recurrent layer
        recurrent = self.recurrent_layer(self.size_hidden, name='recurrent_layer',
                                         weights=W_recurrent,
                                         trainable=False,
                                         return_sequences=True,
                                         dropout_U=self.dropout_recurrent)(embeddings)

        mask = TimeDistributed(Masking(mask_value=0.0))(recurrent)
        
        # add classifier layers
        classifiers = []
        for classifier in self.classifiers:
            try:
                weights = W_classifier[classifier]
            except TypeError:
                weights = None
            classifiers.append(TimeDistributed(Dense(self.output_size[classifier], activation=self.activations[classifier], weights=weights), name=classifier)(mask))

        # create model
        self.model = Model(input=input_layer, output=classifiers)

        self.model.compile(loss=self.loss_functions, optimizer=self.optimizer, metrics=self.metrics, sample_weight_mode='temporal')


    def set_attributes(self):
        """
        Set the classifiers that should be trained and their
        corresponding lossfunctions, metrics and output sizes
        as attributes to the class.
        """
        self.loss_functions = dict([(key, self.loss[key]) for key in self.classifiers])
        self.metrics = dict([(key, self.metrics[key]) for key in self.classifiers])
        self.output_size = dict([(key, self.output_size[key]) for key in self.classifiers])
        self.activations = dict([(key, self.activations[key]) for key in self.classifiers])


    @staticmethod
    def generate_training_data(languages, dmap, digits, classifiers, pad_to=None, format='infix'):

        # generate and shuffle examples
        treebank = mathTreebank(languages, digits=digits)
        random.shuffle(treebank.examples)

        X, Y = Probing.data_from_treebank(treebank, dmap, pad_to=pad_to, classifiers=classifiers, format=format)

        return X, Y

    @staticmethod
    def data_from_treebank(treebank, dmap, pad_to, classifiers, format='infix'):
        """
        Generate test data from a mathTreebank object.
        """
        # create dictionary with outputs
        X, Y = [], dict([(classifier, []) for classifier in classifiers]) 

        # loop over examples
        for expression, answer in treebank.examples:
            expression.get_targets(format=format)
            input_seq = [dmap[i] for i in expression.toString(format).split()]
            X.append(input_seq)
            for classifier in classifiers:
                target = expression.targets[classifier]
                Y[classifier].append(target)
        # pad sequences to have the same length
        assert pad_to is None or len(X[0]) <= pad_to, 'length test is %i, max length is %i. Test sequences should not be truncated' % (len(X[0]), pad_to)
        X_padded = keras.preprocessing.sequence.pad_sequences(X, dtype='int32', maxlen=pad_to)

        # make numpy arrays from Y data
        for output in Y:
            Y[output] = np.array(keras.preprocessing.sequence.pad_sequences(Y[output], maxlen=pad_to))

        X = {'input': X_padded}

        return X, Y

    @staticmethod
    def get_embeddings_layer_id():
        """
        Return embeddings layer ID
        :return (int) id of embeddings layer
        """
        return 1


    @staticmethod
    def get_recurrent_layer_id():
        """
        return recurrent layer ID
        :return (int) id of recurrent layer
        """
        return 2

